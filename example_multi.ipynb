{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn import metrics\n",
    "from sksurv import metrics as skmetrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from ictsurf.dataset import (\n",
    "    get_metabric_dataset_onehot,\n",
    "    get_support2_dataset_onehot,\n",
    "    get_gaussian_dataset,\n",
    "    get_synthetic_dataset_compet,\n",
    "    get_loader\n",
    ")\n",
    "from ictsurf.preprocessing import cut_continuous_time,  CTCutEqualSpacing\n",
    "from ictsurf.eval import *\n",
    "from ictsurf.utils import *\n",
    "from ictsurf.loss import nll_continuous_time_multi_loss_trapezoid\n",
    "from ictsurf.model import MLPTimeEncode\n",
    "from ictsurf.train_utils import test_step\n",
    "from ictsurf import ICTSurF, ICTSurFMulti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1234\n",
    "np.random.seed(random_state)\n",
    "_ = torch.manual_seed(random_state)\n",
    "\n",
    "features, durations, events, true_duration, true_events, risk1_durations, risk2_durations = get_synthetic_dataset_compet()\n",
    "\n",
    "(\n",
    "    features, features_val, \n",
    "    durations, durations_val, \n",
    "    events, events_val, \n",
    "    true_duration, true_duration_val,  \n",
    "    true_events, true_events_val,\n",
    "    risk1_durations, risk1_durations_val,\n",
    "    risk2_durations, risk2_durations_val\n",
    ") = train_test_split(\n",
    "        features, durations, events, true_duration, true_events, risk1_durations, risk2_durations,\n",
    "        test_size=0.15, random_state = random_state, stratify = events)\n",
    "(\n",
    "    features_train, features_test, \n",
    "    durations_train, durations_test, \n",
    "    events_train, events_test, \n",
    "    true_duration_train, true_duration_test,  \n",
    "    true_events_train, true_events_test,\n",
    "    risk1_durations_train, risk1_durations_test,\n",
    "    risk2_durations_train, risk2_durations_test\n",
    ") = train_test_split(\n",
    "        features, durations, events, true_duration, true_events, risk1_durations, risk2_durations,\n",
    "        test_size=0.15, random_state = random_state, stratify = events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_time = np.mean(durations_train)\n",
    "durations_train = durations_train/mean_time\n",
    "durations_val = durations_val/mean_time\n",
    "durations_test = durations_test/mean_time\n",
    "\n",
    "true_duration_test = true_duration_test/mean_time\n",
    "risk1_durations_test = risk1_durations_test/mean_time\n",
    "risk2_durations_test = risk2_durations_test/mean_time\n",
    "\n",
    "scaler =  StandardScaler()\n",
    "features_train = scaler.fit_transform(features_train)\n",
    "features_val = scaler.transform(features_val)\n",
    "features_test = scaler.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 val_loss: 1.3102756737087449 train_loss: 2.2931591445179142\n",
      "epoch 1 val_loss: 1.0283979083556376 train_loss: 1.186303976828263\n",
      "epoch 2 val_loss: 1.0208575996201985 train_loss: 1.0389776042981058\n",
      "epoch 3 val_loss: 0.9740348705756805 train_loss: 0.9855602054985892\n",
      "epoch 4 val_loss: 0.9564489199805402 train_loss: 0.9719841271240279\n",
      "epoch 5 val_loss: 0.940814183694074 train_loss: 0.9599332992614383\n",
      "epoch 6 val_loss: 0.9255820336922354 train_loss: 0.9380448581702131\n",
      "epoch 7 val_loss: 0.9150425334251916 train_loss: 0.9243039664029737\n",
      "epoch 8 val_loss: 0.8943305007624682 train_loss: 0.9004748818580458\n",
      "epoch 9 val_loss: 0.8875684042669245 train_loss: 0.8849469881604531\n",
      "epoch 10 val_loss: 0.861579078241132 train_loss: 0.8655559096170004\n",
      "epoch 11 val_loss: 0.8443714793316354 train_loss: 0.8449284095985881\n",
      "epoch 12 val_loss: 0.8260100898676207 train_loss: 0.8245356187809092\n",
      "epoch 13 val_loss: 0.8084564382099144 train_loss: 0.8067963843835951\n",
      "epoch 14 val_loss: 0.7971647696381313 train_loss: 0.7897753375234702\n",
      "epoch 15 val_loss: 0.7796356080698934 train_loss: 0.7753607199948337\n",
      "epoch 16 val_loss: 0.7678372398307304 train_loss: 0.7672055763479066\n",
      "epoch 17 val_loss: 0.7616246490396668 train_loss: 0.7499840451791785\n",
      "epoch 18 val_loss: 0.7520918856189788 train_loss: 0.7398252834295026\n",
      "epoch 19 val_loss: 0.7439150195835711 train_loss: 0.730475864962223\n",
      "epoch 20 val_loss: 0.7392337618971087 train_loss: 0.7238991577118318\n",
      "epoch 21 val_loss: 0.7338656833797661 train_loss: 0.720362385569655\n",
      "epoch 22 val_loss: 0.730589325253828 train_loss: 0.7148746002218301\n",
      "epoch 23 val_loss: 0.7283523240817932 train_loss: 0.7099428931786427\n",
      "epoch 24 val_loss: 0.7276916906702031 train_loss: 0.7086215613241199\n",
      "epoch 25 val_loss: 0.7251563871814527 train_loss: 0.7056896214783164\n",
      "epoch 26 val_loss: 0.7220292012881587 train_loss: 0.7035781138398937\n",
      "epoch 27 val_loss: 0.7210493435278577 train_loss: 0.7026698342102581\n",
      "epoch 28 val_loss: 0.7199860014173297 train_loss: 0.6998760238080657\n",
      "epoch 29 val_loss: 0.7185578918869967 train_loss: 0.7004530240214022\n",
      "epoch 30 val_loss: 0.7167330538667301 train_loss: 0.6957342799553696\n",
      "epoch 31 val_loss: 0.7158074729713992 train_loss: 0.696187844186092\n",
      "epoch 32 val_loss: 0.7155582147027719 train_loss: 0.6922120279082123\n",
      "epoch 33 val_loss: 0.7158868149081178 train_loss: 0.6943733712111809\n",
      "epoch 34 val_loss: 0.7149927455593708 train_loss: 0.6937075439579273\n",
      "epoch 35 val_loss: 0.7141207575782939 train_loss: 0.6914358029416972\n",
      "epoch 36 val_loss: 0.7149300175119366 train_loss: 0.6896195841767779\n",
      "epoch 37 val_loss: 0.7143049785730887 train_loss: 0.6882093440464524\n",
      "epoch 38 val_loss: 0.7135039467521012 train_loss: 0.6882708151691037\n",
      "epoch 39 val_loss: 0.7130775586967121 train_loss: 0.6854110204436029\n",
      "epoch 40 val_loss: 0.7123478720424498 train_loss: 0.6862212921688645\n",
      "epoch 41 val_loss: 0.7122428626142566 train_loss: 0.6838379212645751\n",
      "epoch 42 val_loss: 0.7123882727973015 train_loss: 0.6859895348401399\n",
      "epoch 43 val_loss: 0.7126353803451109 train_loss: 0.6812183650230343\n",
      "epoch 44 val_loss: 0.7104161667016976 train_loss: 0.6815631940356474\n",
      "epoch 45 val_loss: 0.7106405012690756 train_loss: 0.6847540821543591\n",
      "epoch 46 val_loss: 0.7109964885889137 train_loss: 0.6806302449047786\n",
      "epoch 47 val_loss: 0.7104956922041665 train_loss: 0.6811437312925479\n",
      "epoch 48 val_loss: 0.7101064201377708 train_loss: 0.6801642383157835\n",
      "epoch 49 val_loss: 0.7109384681453875 train_loss: 0.6799559164441334\n",
      "epoch 50 val_loss: 0.7088276011893284 train_loss: 0.6786958419021369\n",
      "epoch 51 val_loss: 0.710379442164533 train_loss: 0.6786026145906672\n",
      "epoch 52 val_loss: 0.7087677683142388 train_loss: 0.6774781850676639\n",
      "epoch 53 val_loss: 0.7092369864396999 train_loss: 0.6765916220098616\n",
      "epoch 54 val_loss: 0.7101202986402864 train_loss: 0.6769037927231979\n",
      "epoch 55 val_loss: 0.7088695693001211 train_loss: 0.6766493693169607\n",
      "epoch 56 val_loss: 0.7093396541826976 train_loss: 0.6763822735952918\n",
      "epoch 57 val_loss: 0.7076174457002142 train_loss: 0.6733145379978852\n",
      "epoch 58 val_loss: 0.7086458369508941 train_loss: 0.6737667036078472\n",
      "epoch 59 val_loss: 0.7125950668267047 train_loss: 0.6739645977113907\n",
      "epoch 60 val_loss: 0.7073090549554383 train_loss: 0.6726252115136131\n",
      "epoch 61 val_loss: 0.7078757982385301 train_loss: 0.6719738775481128\n",
      "epoch 62 val_loss: 0.7085729855238062 train_loss: 0.6729380454397652\n",
      "epoch 63 val_loss: 0.7080905953243573 train_loss: 0.6711635622325598\n",
      "epoch 64 val_loss: 0.7094787516324756 train_loss: 0.6718065011360688\n",
      "epoch 65 val_loss: 0.7071884976020696 train_loss: 0.6713348232294999\n",
      "epoch 66 val_loss: 0.7073175864504315 train_loss: 0.668832663210664\n",
      "epoch 67 val_loss: 0.70783589959321 train_loss: 0.6649251009192667\n",
      "epoch 68 val_loss: 0.7064238025244549 train_loss: 0.669933255760372\n",
      "epoch 69 val_loss: 0.7086227271545629 train_loss: 0.6683495048895571\n",
      "epoch 70 val_loss: 0.7072742446973225 train_loss: 0.6698984342823477\n",
      "epoch 71 val_loss: 0.7093958935685603 train_loss: 0.6650988409959974\n",
      "epoch 72 val_loss: 0.7098260622905038 train_loss: 0.6665820990269593\n",
      "epoch 73 val_loss: 0.7083866943826846 train_loss: 0.6653961820458507\n",
      "epoch 74 val_loss: 0.706825965772983 train_loss: 0.6647186840858959\n",
      "epoch 75 val_loss: 0.708466027376634 train_loss: 0.6638057551316796\n",
      "epoch 76 val_loss: 0.7072681457869656 train_loss: 0.6648222858544691\n",
      "epoch 77 val_loss: 0.7070609612966124 train_loss: 0.6626602111419333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7064238025244549"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add 1 for time feature\n",
    "in_features = features_train.shape[1]+1\n",
    "num_nodes = [64]\n",
    "num_nodes_res = [64]\n",
    "time_dim = 16\n",
    "batch_norm = True\n",
    "dropout = 0.0\n",
    "lr = 0.0002\n",
    "activation = nn.ReLU\n",
    "output_risk = 2\n",
    "batch_size = 256\n",
    "epochs = 10000\n",
    "n_discrete_time = 50\n",
    "patience = 10\n",
    "device = 'cpu'\n",
    "\n",
    "# defined network\n",
    "net = MLPTimeEncode(\n",
    "in_features, num_nodes, num_nodes_res, time_dim=time_dim, batch_norm= batch_norm,\n",
    "dropout=dropout, activation=activation, output_risk = output_risk).float()\n",
    "\n",
    "model = ICTSurFMulti(net).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "model.fit(optimizer, features_train, durations_train, events_train,\n",
    "            features_val, durations_val, events_val,\n",
    "    n_discrete_time = n_discrete_time, patience = patience, device = device,\n",
    "    batch_size=batch_size, epochs=epochs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7206997184291418 0.1143647230445533\n",
      "0.7700174706552777 0.14487319313290456\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# select specific time of interest\n",
    "eval_time = np.quantile(durations_test[events_test == 1], 0.25)\n",
    "\n",
    "time_of_interests = np.array([eval_time]*len(features_test))\n",
    "fake_events = np.array([1]*len(features_test))\n",
    "\n",
    "# create dataloader for evaluation using data processor that already fitted from model\n",
    "test_loader = get_loader(features_test, time_of_interests, fake_events, model.processor ,batch_size=256, fit_y=False)\n",
    "\n",
    "preds = test_step(model, test_loader, device = device)\n",
    "\n",
    "# get hazard\n",
    "hazard = model.pred_to_hazard(preds)\n",
    "\n",
    "# get survival probability\n",
    "# to get survival function we need to integrate the hazard function\n",
    "# to integrate, we need discretized time from dataloader\n",
    "# the discretization time can be access from\n",
    "# test_loader.dataset.extended_data['continuous_times']\n",
    "surv = model.pred_to_surv(preds, test_loader)\n",
    "\n",
    "for risk in [1,2]:\n",
    "    event_risk = np.ones_like(true_events_test)\n",
    "    event_risk[true_events_test != risk] = 0\n",
    "    event_risk = event_risk.astype(bool)\n",
    "    y_true = np.ones_like(true_events_test)\n",
    "    if risk == 1:\n",
    "        y_true[risk1_durations_test > eval_time] = 0\n",
    "    elif risk == 2:\n",
    "        y_true[risk2_durations_test > eval_time] = 0\n",
    "        \n",
    "    surv_at_risk = surv[:, :, risk-1]\n",
    "    surv_at_time = surv_at_risk[:, -1]\n",
    "    c = skmetrics.concordance_index_censored(event_risk, true_duration_test, 1-surv_at_time)[0]\n",
    "    brier = metrics.brier_score_loss(y_true, 1-surv_at_time)\n",
    "    print(c, brier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dream_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
