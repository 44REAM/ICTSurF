{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn import metrics\n",
    "from sksurv import metrics as skmetrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from ictsurf.dataset import (\n",
    "    get_metabric_dataset_onehot,\n",
    "    get_support2_dataset_onehot,\n",
    "    get_gaussian_dataset,\n",
    "    get_synthetic_dataset_compet,\n",
    "    get_loader\n",
    ")\n",
    "from ictsurf.preprocessing import cut_continuous_time,  CTCutEqualSpacing\n",
    "from ictsurf.eval import *\n",
    "from ictsurf.utils import *\n",
    "from ictsurf.loss import nll_continuous_time_multi_loss_trapezoid\n",
    "from ictsurf.model import MLPTimeEncode\n",
    "from ictsurf.train_utils import test_step\n",
    "from ictsurf import ICTSurF, ICTSurFMulti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1234\n",
    "np.random.seed(random_state)\n",
    "_ = torch.manual_seed(random_state)\n",
    "\n",
    "features, durations, events = get_support2_dataset_onehot()\n",
    "\n",
    "features, features_val, durations, durations_val, events, events_val = train_test_split(\n",
    "            features, durations, events, test_size=0.15, random_state = random_state, stratify = events)\n",
    "\n",
    "features_train, features_test, durations_train, durations_test, events_train, events_test = train_test_split(\n",
    "            features, durations, events, test_size=0.15, random_state = random_state, stratify = events)\n",
    "\n",
    "# remove the samples with durations greater than the maximum duration in the training set\n",
    "# because c-index from library sksurv cannot handle this case\n",
    "while durations_train.max()<=durations_test.max():\n",
    "    test_index_max = durations_test.argmax()\n",
    "    durations_test = deepcopy(np.delete(durations_test, test_index_max))\n",
    "    features_test = deepcopy(np.delete(features_test, test_index_max, axis = 0))\n",
    "    events_test = deepcopy(np.delete(events_test, test_index_max))\n",
    "    \n",
    "while durations_train.max()<=durations_val.max():\n",
    "    test_index_max = durations_val.argmax()\n",
    "    durations_val = deepcopy(np.delete(durations_val, test_index_max))\n",
    "    features_val = deepcopy(np.delete(features_val, test_index_max, axis = 0))\n",
    "    events_val = deepcopy(np.delete(events_val, test_index_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_time = np.mean(durations_train)\n",
    "durations_train = durations_train/mean_time\n",
    "durations_val = durations_val/mean_time\n",
    "durations_test = durations_test/mean_time\n",
    "\n",
    "scaler =  StandardScaler()\n",
    "\n",
    "features_train = scaler.fit_transform(features_train)\n",
    "features_val = scaler.transform(features_val)\n",
    "features_test = scaler.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 val_loss: 0.5890185779173982 train_loss: 0.6010431903096392\n",
      "epoch 6 val_loss: 0.555299671404275 train_loss: 0.5621553739853233\n",
      "epoch 7 val_loss: 0.5302227970668731 train_loss: 0.5339830898330713\n",
      "epoch 8 val_loss: 0.5080168172993881 train_loss: 0.5044601295172714\n",
      "epoch 9 val_loss: 0.4859957410820004 train_loss: 0.481226466366328\n",
      "epoch 10 val_loss: 0.4683984379812615 train_loss: 0.45970528679939804\n",
      "epoch 11 val_loss: 0.4523657756157259 train_loss: 0.43712887861902894\n",
      "epoch 12 val_loss: 0.4387495931680033 train_loss: 0.423800971262455\n",
      "epoch 13 val_loss: 0.42761736930733213 train_loss: 0.40689455899073684\n",
      "epoch 14 val_loss: 0.416024483478575 train_loss: 0.39290452062150516\n",
      "epoch 15 val_loss: 0.40664274824950375 train_loss: 0.38289311330796805\n",
      "epoch 16 val_loss: 0.3985128894679264 train_loss: 0.3690381046117367\n",
      "epoch 17 val_loss: 0.3880805564715657 train_loss: 0.35997805692680374\n",
      "epoch 18 val_loss: 0.3803891124037417 train_loss: 0.3509233159741795\n",
      "epoch 19 val_loss: 0.3748578027725879 train_loss: 0.3416515292260341\n",
      "epoch 20 val_loss: 0.3684975948245249 train_loss: 0.33615138439085224\n",
      "epoch 21 val_loss: 0.36187130049726896 train_loss: 0.3221959869319482\n",
      "epoch 22 val_loss: 0.35723168989580883 train_loss: 0.3161033792543446\n",
      "epoch 23 val_loss: 0.35187282086204735 train_loss: 0.3182239705673084\n",
      "epoch 24 val_loss: 0.3478994911282322 train_loss: 0.30588138113387225\n",
      "epoch 25 val_loss: 0.3437911233533983 train_loss: 0.2986367363250339\n",
      "epoch 26 val_loss: 0.33981546045490485 train_loss: 0.2951869473782925\n",
      "epoch 27 val_loss: 0.33597936911916715 train_loss: 0.2874169857085925\n",
      "epoch 28 val_loss: 0.33294870028867846 train_loss: 0.28547406406537634\n",
      "epoch 29 val_loss: 0.32997250760919417 train_loss: 0.2766775010184966\n",
      "epoch 30 val_loss: 0.32709704166695025 train_loss: 0.27796052913425584\n",
      "epoch 31 val_loss: 0.32452587302550784 train_loss: 0.2728515432879675\n",
      "epoch 32 val_loss: 0.32273809081130517 train_loss: 0.26939627834079716\n",
      "epoch 33 val_loss: 0.3201867572321488 train_loss: 0.2721636186901599\n",
      "epoch 34 val_loss: 0.3175241486684579 train_loss: 0.26097156566434715\n",
      "epoch 35 val_loss: 0.3155562275840932 train_loss: 0.26086528899476596\n",
      "epoch 36 val_loss: 0.3138938180542449 train_loss: 0.2594142032324211\n",
      "epoch 37 val_loss: 0.31235131283950135 train_loss: 0.24703928226202237\n",
      "epoch 38 val_loss: 0.31084024232300217 train_loss: 0.2506678372675058\n",
      "epoch 39 val_loss: 0.3091939698861127 train_loss: 0.24492526039448212\n",
      "epoch 40 val_loss: 0.3078105565287028 train_loss: 0.24589986810338513\n",
      "epoch 41 val_loss: 0.3066896037730908 train_loss: 0.23515099937508804\n",
      "epoch 42 val_loss: 0.30493478671415747 train_loss: 0.23729737982053592\n",
      "epoch 43 val_loss: 0.3037185773235926 train_loss: 0.23910251728417753\n",
      "epoch 44 val_loss: 0.3024763508296315 train_loss: 0.23057959541751769\n",
      "epoch 45 val_loss: 0.30146583533491284 train_loss: 0.2331295592828829\n",
      "epoch 46 val_loss: 0.3014027291851766 train_loss: 0.22790254946065985\n",
      "epoch 47 val_loss: 0.2999500245078156 train_loss: 0.22544210006108822\n",
      "epoch 48 val_loss: 0.29973275311246583 train_loss: 0.21726738975473928\n",
      "epoch 49 val_loss: 0.2981220166830713 train_loss: 0.2210578469655645\n",
      "epoch 50 val_loss: 0.2976365897797653 train_loss: 0.21658953828704136\n",
      "epoch 51 val_loss: 0.2964746597423481 train_loss: 0.21939158145540635\n",
      "epoch 52 val_loss: 0.29552014156488443 train_loss: 0.21956283563848544\n",
      "epoch 53 val_loss: 0.2958281393202545 train_loss: 0.21278064694623317\n",
      "epoch 54 val_loss: 0.2956761855581566 train_loss: 0.21530027035705776\n",
      "epoch 55 val_loss: 0.29472244342454146 train_loss: 0.2059512075438878\n",
      "epoch 56 val_loss: 0.2936934286182803 train_loss: 0.20544175418731137\n",
      "epoch 57 val_loss: 0.2928334851367459 train_loss: 0.20349972044610948\n",
      "epoch 58 val_loss: 0.29182934859600757 train_loss: 0.2032221759016919\n",
      "epoch 59 val_loss: 0.29224889563224804 train_loss: 0.19487384988963\n",
      "epoch 60 val_loss: 0.2920625940445522 train_loss: 0.19515264573686733\n",
      "epoch 61 val_loss: 0.29106896826522327 train_loss: 0.19205089350303214\n",
      "epoch 62 val_loss: 0.29290056156157757 train_loss: 0.19532794230748624\n",
      "epoch 63 val_loss: 0.29106537556154594 train_loss: 0.1930778852232601\n",
      "epoch 64 val_loss: 0.2912540079104332 train_loss: 0.18480762996566774\n",
      "epoch 65 val_loss: 0.2902852862957375 train_loss: 0.17724009006496183\n",
      "epoch 66 val_loss: 0.2898745700229161 train_loss: 0.18168580924085412\n",
      "epoch 67 val_loss: 0.2891680790417574 train_loss: 0.18177570449215577\n",
      "epoch 68 val_loss: 0.28963490223739535 train_loss: 0.17750972591607891\n",
      "epoch 69 val_loss: 0.2902774931735369 train_loss: 0.17265064768870622\n",
      "epoch 70 val_loss: 0.2909104402978184 train_loss: 0.17779165958178939\n",
      "epoch 71 val_loss: 0.2890810416081957 train_loss: 0.17306348849261322\n",
      "epoch 72 val_loss: 0.28961827009727287 train_loss: 0.1709164793430749\n",
      "epoch 73 val_loss: 0.290680822749518 train_loss: 0.17065360857275688\n",
      "epoch 74 val_loss: 0.29200848032931126 train_loss: 0.17237032010963704\n",
      "epoch 75 val_loss: 0.28867970095258444 train_loss: 0.1680938625170269\n",
      "epoch 76 val_loss: 0.28920753814592953 train_loss: 0.1681375507137055\n",
      "epoch 77 val_loss: 0.2883737283459465 train_loss: 0.1557984019603983\n",
      "epoch 78 val_loss: 0.29012830761619135 train_loss: 0.1616964540942515\n",
      "epoch 79 val_loss: 0.29012530403745934 train_loss: 0.15595671512955447\n",
      "epoch 80 val_loss: 0.2900387814724132 train_loss: 0.15542479579389312\n",
      "epoch 81 val_loss: 0.28903527131564416 train_loss: 0.1487202794331768\n",
      "epoch 82 val_loss: 0.2907504711549541 train_loss: 0.15150001773279734\n",
      "epoch 83 val_loss: 0.29130971371324166 train_loss: 0.1491293136491294\n",
      "epoch 84 val_loss: 0.29129393561905537 train_loss: 0.1460427362100721\n",
      "epoch 85 val_loss: 0.2923361610564615 train_loss: 0.13990037729348778\n",
      "epoch 86 val_loss: 0.2916264610595374 train_loss: 0.1371102922068535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2883737283459465"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add 1 for time feature\n",
    "in_features = features_train.shape[1]+1\n",
    "num_nodes = [64]\n",
    "num_nodes_res = [64]\n",
    "time_dim = 16\n",
    "batch_norm = True\n",
    "dropout = 0.0\n",
    "lr = 0.0002\n",
    "activation = nn.ReLU\n",
    "output_risk = 1\n",
    "batch_size = 256\n",
    "epochs = 10000\n",
    "n_discrete_time = 50\n",
    "patience = 10\n",
    "device = 'cpu'\n",
    "\n",
    "# defined network\n",
    "net = MLPTimeEncode(\n",
    "in_features, num_nodes, num_nodes_res, time_dim=time_dim, batch_norm= batch_norm,\n",
    "dropout=dropout, activation=activation, output_risk = output_risk).float()\n",
    "\n",
    "# # or defined your own network\n",
    "# class CustomNet(nn.Module):\n",
    "#     def __init__(self, in_features, output_risk = 1):\n",
    "#         super().__init__()\n",
    "#         self.output_risk = output_risk\n",
    "#         self.linear = nn.Linear(in_features, 1)\n",
    "    \n",
    "#     def forward(self, input):\n",
    "        \n",
    "#         # the input in shape of (batch, time_step, in_features)\n",
    "#         # the last features is the time feature\n",
    "#         time_step = input.shape[1]\n",
    "#         input = input.view(-1, input.shape[-1])\n",
    "#         out = self.linear(input)\n",
    "\n",
    "#         if self.output_risk == 1:\n",
    "\n",
    "#             return out.view(-1, time_step)\n",
    "#         return out.reshape(-1, time_step, 1)\n",
    "# net = CustomNet(in_features)\n",
    "\n",
    "model = ICTSurF(net).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "model.fit(optimizer, features_train, durations_train, events_train,\n",
    "            features_val, durations_val, events_val,\n",
    "    n_discrete_time = n_discrete_time, patience = patience, device = device,\n",
    "    batch_size=batch_size, epochs=epochs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate using our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>timepoint</th>\n",
       "      <th>c_index</th>\n",
       "      <th>brier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ICTSurf_loss.pth</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.833771</td>\n",
       "      <td>0.115548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ICTSurf_loss.pth</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.783476</td>\n",
       "      <td>0.158197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ICTSurf_loss.pth</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.181603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model  timepoint   c_index     brier\n",
       "0  ICTSurf_loss.pth       0.25  0.833771  0.115548\n",
       "0  ICTSurf_loss.pth       0.50  0.783476  0.158197\n",
       "0  ICTSurf_loss.pth       0.75  0.749687  0.181603"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_df = model.evaluate(\n",
    "            features_test, durations_test, events_test, \n",
    "            quantile_evals = [0.25, 0.5, 0.75],\n",
    "            interpolation = True, device = device)\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# select specific time of interest\n",
    "eval_time = np.quantile(durations_test[events_test == 1], 0.25)\n",
    "\n",
    "time_of_interests = np.array([eval_time]*len(features_test))\n",
    "fake_events = np.array([1]*len(features_test))\n",
    "\n",
    "# create dataloader for evaluation using data processor that already fitted from model\n",
    "test_loader = get_loader(features_test, time_of_interests, fake_events, model.processor ,batch_size=256, fit_y=False)\n",
    "\n",
    "preds = test_step(model, test_loader, device = device)\n",
    "\n",
    "# get hazard\n",
    "hazard = model.pred_to_hazard(preds)\n",
    "\n",
    "# get survival probability\n",
    "# to get survival function we need to integrate the hazard function\n",
    "# to integrate, we need discretized time from dataloader\n",
    "# the discretization time can be access from\n",
    "# test_loader.dataset.extended_data['continuous_times']\n",
    "surv = model.pred_to_surv(preds, test_loader)\n",
    "\n",
    "# then using survival probability for your evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dream_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
